{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPD0pm02RWlFXJAspzE42bX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bythyag/smolgpt/blob/main/GPT_scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hal6Qcf7XE_N",
        "outputId": "78c6495e-39f2-4f45-8ff4-378482bb5a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader all -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 1: Setup and Google Drive Mount\n",
        "#------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast # For mixed precision\n",
        "\n",
        "import math\n",
        "import os\n",
        "import requests # For downloading data\n",
        "import nltk # For word tokenization\n",
        "from collections import Counter\n",
        "import pickle # To save/load vocabulary\n",
        "import time\n",
        "from tqdm import tqdm # Progress bar\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/gpt2_scratch_wordlevel_tiny\" # CHANGE AS NEEDED\n",
        "if not os.path.exists(DRIVE_SAVE_DIR):\n",
        "    os.makedirs(DRIVE_SAVE_DIR)\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download NLTK tokenizer data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "print(\"NLTK punkt downloaded.\")\n",
        "\n",
        "print(\"Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PzWxJUpXInH",
        "outputId": "e73f2e4c-71dd-45b2-aafe-5ecc96d2d77e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "NLTK punkt downloaded.\n",
            "Setup Complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 2: Configuration\n",
        "#------------------------------------\n",
        "\n",
        "# --- Model Hyperparameters --- Choose one preset ---\n",
        "# Preset: 'nano' (very small, fast to train, low quality)\n",
        "#n_layer = 3\n",
        "#n_head = 3\n",
        "#n_embd = 48\n",
        "\n",
        "# Preset: 'micro' (slightly larger nano)\n",
        "#n_layer = 4\n",
        "#n_head = 4\n",
        "#n_embd = 128\n",
        "\n",
        "# Preset: 'tiny' (closer to small models, feasible on T4)\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384 # Embedding dimension must be divisible by n_head\n",
        "\n",
        "# Preset: 'small' (might push T4 limits, reduce batch_size if needed)\n",
        "# n_layer = 12\n",
        "# n_head = 12\n",
        "# n_embd = 768 # Original GPT-2 small size - LIKELY TOO BIG FOR T4 FREE TIER\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "block_size = 128      # Max context length for predictions (sequence length)\n",
        "batch_size = 32       # How many sequences process in parallel? Reduce if OOM.\n",
        "max_iters = 5000      # Total training iterations\n",
        "eval_interval = 250   # How often to evaluate on validation set\n",
        "learning_rate = 3e-4  # Learning rate\n",
        "eval_iters = 100      # Number of batches to average for validation loss\n",
        "dropout = 0.1         # Dropout rate\n",
        "use_amp = True        # Use Automatic Mixed Precision (highly recommended on T4)\n",
        "\n",
        "# --- Data ---\n",
        "DATA_URL = \"https://www.gutenberg.org/files/1661/1661-0.txt\" # Sherlock Holmes\n",
        "DATA_PATH = \"sherlock_holmes.txt\"\n",
        "VOCAB_PATH = os.path.join(DRIVE_SAVE_DIR, \"word_vocab.pkl\")\n",
        "TRAIN_MODEL_PATH = os.path.join(DRIVE_SAVE_DIR, \"gpt2_word_level.pth\")\n",
        "MIN_WORD_FREQ = 3     # Minimum frequency for a word to be included in vocab\n",
        "\n",
        "# Derived parameters\n",
        "assert n_embd % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "config = {\n",
        "    'n_layer': n_layer,\n",
        "    'n_head': n_head,\n",
        "    'n_embd': n_embd,\n",
        "    'block_size': block_size,\n",
        "    'batch_size': batch_size,\n",
        "    'max_iters': max_iters,\n",
        "    'eval_interval': eval_interval,\n",
        "    'learning_rate': learning_rate,\n",
        "    'eval_iters': eval_iters,\n",
        "    'dropout': dropout,\n",
        "    'use_amp': use_amp,\n",
        "    'vocab_size': -1, # Will be set after data loading\n",
        "    'device': device,\n",
        "    'min_word_freq': MIN_WORD_FREQ,\n",
        "    'vocab_path': VOCAB_PATH,\n",
        "    'model_path': TRAIN_MODEL_PATH\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, val in config.items():\n",
        "    print(f\"{key}: {val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX1dB4UOXO4e",
        "outputId": "3410914c-c7fe-40fb-db83-c40bee1c4c39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "n_layer: 6\n",
            "n_head: 6\n",
            "n_embd: 384\n",
            "block_size: 128\n",
            "batch_size: 32\n",
            "max_iters: 5000\n",
            "eval_interval: 250\n",
            "learning_rate: 0.0003\n",
            "eval_iters: 100\n",
            "dropout: 0.1\n",
            "use_amp: True\n",
            "vocab_size: -1\n",
            "device: cuda\n",
            "min_word_freq: 3\n",
            "vocab_path: /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/word_vocab.pkl\n",
            "model_path: /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 3: Data Preparation\n",
        "#------------------------------------\n",
        "\n",
        "# --- Download Data ---\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"Downloading data from {DATA_URL}...\")\n",
        "    try:\n",
        "        response = requests.get(DATA_URL)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        # The downloaded text often has BOM (Byte Order Mark) and needs decoding\n",
        "        text = response.content.decode('utf-8-sig')\n",
        "        # Basic cleaning: remove Gutenberg header/footer (heuristic)\n",
        "        start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "        start_idx = text.find(start_marker)\n",
        "        if start_idx != -1:\n",
        "            start_idx += len(start_marker)\n",
        "            text = text[start_idx:]\n",
        "        end_idx = text.find(end_marker)\n",
        "        if end_idx != -1:\n",
        "            text = text[:end_idx]\n",
        "\n",
        "        text = text.strip() # Remove leading/trailing whitespace\n",
        "        print(f\"Data downloaded and saved to {DATA_PATH}. Length: {len(text)} characters.\")\n",
        "        with open(DATA_PATH, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading data: {e}\")\n",
        "        # Handle error appropriately, maybe exit or use cached data if available\n",
        "        text = \"\" # Ensure 'text' exists even on failure\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data processing: {e}\")\n",
        "        text = \"\"\n",
        "else:\n",
        "    print(f\"Data file {DATA_PATH} already exists.\")\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    print(f\"Loaded data from {DATA_PATH}. Length: {len(text)} characters.\")\n",
        "\n",
        "\n",
        "# --- Tokenization & Vocabulary ---\n",
        "print(\"Tokenizing text...\")\n",
        "# Use NLTK for word tokenization. Consider lowercasing for smaller vocab.\n",
        "tokens = nltk.word_tokenize(text.lower()) # Lowercasing reduces vocab size\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "\n",
        "# Build Vocabulary\n",
        "print(\"Building vocabulary...\")\n",
        "word_counts = Counter(tokens)\n",
        "# Keep words that appear at least MIN_WORD_FREQ times\n",
        "filtered_word_counts = {word: count for word, count in word_counts.items() if count >= config['min_word_freq']}\n",
        "\n",
        "# Create mapping from word to integer index\n",
        "# Add special tokens: PAD (optional but good practice) and UNK\n",
        "# We won't explicitly use PAD here for simplicity with causal masking,\n",
        "# but UNK is important.\n",
        "# <PAD> = 0 , <UNK> = 1\n",
        "# Start actual words from index 2\n",
        "# Note: For generation, sometimes people add <SOS> and <EOS> (Start/End of Sentence)\n",
        "# but for standard GPT pretraining on long texts, they are less common.\n",
        "stoi = {word: i+2 for i, word in enumerate(filtered_word_counts)}\n",
        "stoi['<PAD>'] = 0 # Padding token index\n",
        "stoi['<UNK>'] = 1 # Unknown token index\n",
        "\n",
        "itos = {i: word for word, i in stoi.items()}\n",
        "vocab_size = len(stoi)\n",
        "config['vocab_size'] = vocab_size # Update config\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Saving vocabulary to {config['vocab_path']}...\")\n",
        "with open(config['vocab_path'], 'wb') as f:\n",
        "    pickle.dump({'stoi': stoi, 'itos': itos}, f)\n",
        "print(\"Vocabulary saved.\")\n",
        "\n",
        "# --- Encode/Decode Functions ---\n",
        "def encode(text_string):\n",
        "    words = nltk.word_tokenize(text_string.lower())\n",
        "    return [stoi.get(word, stoi['<UNK>']) for word in words]\n",
        "\n",
        "def decode(indices):\n",
        "    return ' '.join([itos.get(i, '?') for i in indices]) # Use '?' for unexpected indices\n",
        "\n",
        "# --- Create Data Tensors ---\n",
        "print(\"Encoding entire dataset...\")\n",
        "full_data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"Encoded data shape: {full_data.shape}\")\n",
        "\n",
        "# Split data into train and validation sets\n",
        "n = len(full_data)\n",
        "train_data = full_data[:int(n*0.9)]\n",
        "val_data = full_data[int(n*0.9):]\n",
        "\n",
        "print(f\"Train set size: {len(train_data)} tokens\")\n",
        "print(f\"Validation set size: {len(val_data)} tokens\")\n",
        "\n",
        "# --- Dataset Class ---\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # We can start a sequence at almost any point\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Grab a chunk of data for context (x) and target (y)\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        x = chunk[:-1]\n",
        "        y = chunk[1:]\n",
        "        return x, y\n",
        "\n",
        "# Example usage (optional):\n",
        "# test_dataset = TextDataset(train_data, config['block_size'])\n",
        "# x_ex, y_ex = test_dataset[0]\n",
        "# print(\"Example x:\", x_ex)\n",
        "# print(\"Example y:\", y_ex)\n",
        "# print(\"Decoded x:\", decode(x_ex.tolist()))\n",
        "# print(\"Decoded y:\", decode(y_ex.tolist()))\n",
        "\n",
        "print(\"Data Preparation Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2r3f8LPXotg",
        "outputId": "38a591ae-109e-42ae-d722-1361f5d66747"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data file sherlock_holmes.txt already exists.\n",
            "Loaded data from sherlock_holmes.txt. Length: 581421 characters.\n",
            "Tokenizing text...\n",
            "Total tokens: 128528\n",
            "Building vocabulary...\n",
            "Vocabulary size: 3291\n",
            "Saving vocabulary to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/word_vocab.pkl...\n",
            "Vocabulary saved.\n",
            "Encoding entire dataset...\n",
            "Encoded data shape: torch.Size([128528])\n",
            "Train set size: 115675 tokens\n",
            "Validation set size: 12853 tokens\n",
            "Data Preparation Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 4: GPT-2 Model Components\n",
        "#------------------------------------\n",
        "\n",
        "# --- Layer Normalization ---\n",
        "# Simplified LayerNorm implementation for understanding,\n",
        "# but using nn.LayerNorm is standard and often optimized.\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "    def __init__(self, ndim, bias=True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Calculate mean and variance along the last dimension (embedding dimension)\n",
        "        # Keep dimension for broadcasting\n",
        "        mean = input.mean(dim=-1, keepdim=True)\n",
        "        var = input.var(dim=-1, keepdim=True, unbiased=False) # Use population variance\n",
        "\n",
        "        # Normalize\n",
        "        # Add eps for numerical stability (avoid division by zero)\n",
        "        normalized_input = (input - mean) / torch.sqrt(var + 1e-5)\n",
        "\n",
        "        # Scale and shift\n",
        "        output = normalized_input * self.weight\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        return output\n",
        "\n",
        "# --- Causal Self-Attention Head ---\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config['n_embd'] % config['n_head'] == 0\n",
        "        # K, Q, V projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(config['dropout'])\n",
        "        self.resid_dropout = nn.Dropout(config['dropout'])\n",
        "        # Causal mask\n",
        "        # Not a parameter, assigned to buffer\n",
        "        # Uses a lower triangular matrix for masking future positions\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
        "                                      .view(1, 1, config['block_size'], config['block_size']))\n",
        "        self.n_head = config['n_head']\n",
        "        self.n_embd = config['n_embd']\n",
        "        self.dropout = config['dropout']\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # q, k, v shape: (B, n_head, T, head_size)\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        head_size = C // self.n_head\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        # Manual implementation of attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_size))\n",
        "        # Apply causal mask (upper triangular part is masked)\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        # Apply softmax\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        # Apply dropout to attention weights\n",
        "        att = self.attn_dropout(att)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        # Re-assemble all head outputs side by side\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # (B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "# --- Feed Forward Network (MLP) ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd'])\n",
        "        # GELU activation function is standard in GPT-2\n",
        "        # For compatibility or simplicity, ReLU could be used but GELU is preferred\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd'])\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# --- Transformer Block ---\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config['n_embd']) # Or nn.LayerNorm(config['n_embd'])\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config['n_embd']) # Or nn.LayerNorm(config['n_embd'])\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the transformer block\n",
        "        # Residual connections are crucial\n",
        "        x = x + self.attn(self.ln_1(x)) # Attention path\n",
        "        x = x + self.mlp(self.ln_2(x))  # MLP path\n",
        "        return x\n",
        "\n",
        "print(\"Model Components Defined (LayerNorm, CausalSelfAttention, MLP, Block).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQMe0KtMXt_w",
        "outputId": "8372fb2c-85c9-43a9-f456-763a2e466e45"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Components Defined (LayerNorm, CausalSelfAttention, MLP, Block).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 5: Full GPT-2 Model\n",
        "#------------------------------------\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            # Token embeddings\n",
        "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
        "            # Positional embeddings (learned)\n",
        "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
        "            # Dropout after embedding + positional encoding\n",
        "            drop = nn.Dropout(config['dropout']),\n",
        "            # Stack of transformer blocks\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
        "            # Final layer normalization before the output head\n",
        "            ln_f = LayerNorm(config['n_embd']), # Or nn.LayerNorm(config['n_embd'])\n",
        "        ))\n",
        "        # Language modeling head (maps embeddings to vocabulary logits)\n",
        "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Tie the weights between the token embeddings and the final linear layer\n",
        "        # This improves performance and reduces parameters\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize weights (important for transformer stability)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
        "\n",
        "        # Report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Model Parameter Count: {n_params/1e6:.2f} M\")\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        # Initialize Linear and Embedding layers\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size() # Batch size, sequence length\n",
        "        assert t <= self.config['block_size'], f\"Cannot forward sequence of length {t}, block size is only {self.config['block_size']}\"\n",
        "\n",
        "        # --- Forward pass through the transformer ---\n",
        "        # 1. Get token embeddings\n",
        "        tok_emb = self.transformer.wte(idx) # Shape: (b, t, n_embd)\n",
        "\n",
        "        # 2. Get positional embeddings\n",
        "        # Create position IDs: tensor of [0, 1, ..., t-1]\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # Shape: (1, t)\n",
        "        pos_emb = self.transformer.wpe(pos) # Shape: (1, t, n_embd)\n",
        "\n",
        "        # 3. Add token and positional embeddings\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # 4. Pass through transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        # 5. Final layer normalization\n",
        "        x = self.transformer.ln_f(x) # Shape: (b, t, n_embd)\n",
        "\n",
        "        # --- Language Modeling Head ---\n",
        "        if targets is not None:\n",
        "            # If we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x) # Shape: (b, t, vocab_size)\n",
        "            # Calculate loss using cross-entropy\n",
        "            # Need to reshape for CrossEntropyLoss: expects (N, C) and (N)\n",
        "            # N = b * t, C = vocab_size\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0) # Ignore PAD token if targets contain it\n",
        "        else:\n",
        "            # Inference-time configuration: only forward the lm_head on the very last position\n",
        "            # This is slightly more efficient during generation\n",
        "            logits = self.lm_head(x[:, [-1], :]) # Note: using list [-1] keeps the T dimension\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # --- Generation Method ---\n",
        "    @torch.no_grad() # IMPORTANT: Disable gradient calculation during generation\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # If the sequence context is growing too long, crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
        "\n",
        "            # Forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond) # We don't need the loss here\n",
        "\n",
        "            # Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature # Shape: (b, vocab_size)\n",
        "\n",
        "            # Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                # Set logits not in the top k to -infinity\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # Shape: (b, vocab_size)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # Shape: (b, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # Shape: (b, t+1)\n",
        "\n",
        "        self.train() # Set model back to train mode if needed later\n",
        "        return idx\n",
        "\n",
        "# --- Instantiate the model ---\n",
        "# Ensure vocab size is set in config\n",
        "if config['vocab_size'] == -1:\n",
        "     raise ValueError(\"Vocabulary size not set. Run Snippet 3 first.\")\n",
        "\n",
        "model = GPT(config)\n",
        "model.to(device)\n",
        "\n",
        "print(\"GPT Model Instantiated.\")\n",
        "# Optional: Print model structure\n",
        "# print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJnsPxhzXzUG",
        "outputId": "8365c097-d17a-4e75-a832-5117446d2751"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Parameter Count: 11.96 M\n",
            "GPT Model Instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 6: Training Loop\n",
        "#------------------------------------\n",
        "\n",
        "# --- DataLoader ---\n",
        "train_dataset = TextDataset(train_data, config['block_size'])\n",
        "val_dataset = TextDataset(val_data, config['block_size'])\n",
        "\n",
        "# Use pin_memory=True if data fits in CPU RAM and using GPU for faster transfer\n",
        "# num_workers > 0 can speed up data loading but might cause issues in Colab sometimes\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --- Optimizer ---\n",
        "# AdamW is a standard optimizer for transformers\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "# --- Mixed Precision Scaler ---\n",
        "# Used only if config['use_amp'] is True\n",
        "scaler = GradScaler(enabled=config['use_amp'])\n",
        "\n",
        "# --- Learning Rate Scheduler (Optional but recommended) ---\n",
        "# Example: Cosine decay schedule\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['max_iters'], eta_min=config['learning_rate']/10)\n",
        "\n",
        "# --- Estimate Validation Loss Function ---\n",
        "@torch.no_grad() # Disable gradient calculation for evaluation\n",
        "def estimate_loss(model_to_eval, eval_iters):\n",
        "    out = {}\n",
        "    model_to_eval.eval() # Set model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        loader = train_loader if split == 'train' else val_loader\n",
        "        loader_iter = iter(loader)\n",
        "        for k in range(eval_iters):\n",
        "            try:\n",
        "                X, Y = next(loader_iter)\n",
        "            except StopIteration: # Reset iterator if needed\n",
        "                 loader_iter = iter(loader)\n",
        "                 X, Y = next(loader_iter)\n",
        "\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            with autocast(enabled=config['use_amp']): # Enable AMP context\n",
        "                 logits, loss = model_to_eval(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model_to_eval.train() # Set model back to training mode\n",
        "    return out\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(f\"Starting training for {config['max_iters']} iterations...\")\n",
        "best_val_loss = float('inf')\n",
        "start_time = time.time()\n",
        "\n",
        "# Resume from checkpoint if exists\n",
        "if os.path.exists(config['model_path']):\n",
        "    print(f\"Resuming training from checkpoint: {config['model_path']}\")\n",
        "    checkpoint = torch.load(config['model_path'], map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # Careful: Optimizer state might need adjustment if hyperparameters changed\n",
        "    # For simplicity here, we re-initialize optimizer, but loading it is better practice\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    # best_val_loss = checkpoint.get('best_val_loss', float('inf')) # Load best loss if saved\n",
        "    # start_iter = checkpoint.get('iter', 0) + 1 # Resume iteration count\n",
        "    print(\"Loaded model weights. Re-initializing optimizer.\")\n",
        "else:\n",
        "    print(\"Starting training from scratch.\")\n",
        "    start_iter = 0\n",
        "\n",
        "\n",
        "# Use an iterator for the training data loader\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "for iter_num in range(start_iter, config['max_iters']):\n",
        "    # Every eval_interval evaluate loss on train and val sets\n",
        "    if iter_num % config['eval_interval'] == 0 or iter_num == config['max_iters'] - 1:\n",
        "        losses = estimate_loss(model, config['eval_iters'])\n",
        "        current_time = time.time()\n",
        "        elapsed_time = current_time - start_time\n",
        "        print(f\"Step {iter_num}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}, Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save model checkpoint if validation loss improved\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            print(f\"New best validation loss: {best_val_loss:.4f}. Saving model...\")\n",
        "            checkpoint = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(), # Save optimizer state too\n",
        "                'config': config,\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'iter': iter_num\n",
        "            }\n",
        "            torch.save(checkpoint, config['model_path'])\n",
        "            print(f\"Model saved to {config['model_path']}\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    try:\n",
        "        X_batch, Y_batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        # Epoch finished, start new epoch\n",
        "        train_iter = iter(train_loader)\n",
        "        X_batch, Y_batch = next(train_iter)\n",
        "\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Forward pass & loss calculation, using mixed precision context\n",
        "    with autocast(enabled=config['use_amp']):\n",
        "        logits, loss = model(X_batch, Y_batch)\n",
        "\n",
        "    # Backward pass & optimization\n",
        "    optimizer.zero_grad(set_to_none=True) # More efficient zeroing\n",
        "    scaler.scale(loss).backward()         # Scale loss for mixed precision backward pass\n",
        "    # Optional: Gradient Clipping - helps prevent exploding gradients\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    scaler.step(optimizer)                # Optimizer step (unscales gradients internally)\n",
        "    scaler.update()                       # Update scaler for next iteration\n",
        "\n",
        "    # Update learning rate (if using scheduler)\n",
        "    # scheduler.step()\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTraining finished in {end_time - start_time:.2f} seconds.\")\n",
        "print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
        "print(f\"Final model saved to {config['model_path']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVREIYH6X3mM",
        "outputId": "f7798703-23ed-45d1-f720-c6abba9251b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 5000 iterations...\n",
            "Starting training from scratch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5a45c5a8127e>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=config['use_amp'])\n",
            "<ipython-input-13-5a45c5a8127e>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=config['use_amp']): # Enable AMP context\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Train loss 8.2298, Val loss 8.2239, Time: 4.00s\n",
            "New best validation loss: 8.2239. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5a45c5a8127e>:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=config['use_amp']):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250: Train loss 4.2291, Val loss 4.5362, Time: 25.53s\n",
            "New best validation loss: 4.5362. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n",
            "Step 500: Train loss 3.7176, Val loss 4.4019, Time: 45.14s\n",
            "New best validation loss: 4.4019. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n",
            "Step 750: Train loss 3.1563, Val loss 4.4389, Time: 64.19s\n",
            "Step 1000: Train loss 2.4839, Val loss 4.6733, Time: 82.29s\n",
            "Step 1250: Train loss 1.7021, Val loss 5.0798, Time: 100.41s\n",
            "Step 1500: Train loss 1.0136, Val loss 5.5482, Time: 118.78s\n",
            "Step 1750: Train loss 0.5694, Val loss 6.0136, Time: 137.16s\n",
            "Step 2000: Train loss 0.3424, Val loss 6.4148, Time: 156.01s\n",
            "Step 2250: Train loss 0.2554, Val loss 6.7523, Time: 174.22s\n",
            "Step 2500: Train loss 0.2120, Val loss 6.9744, Time: 192.41s\n",
            "Step 2750: Train loss 0.1912, Val loss 7.1061, Time: 212.28s\n",
            "Step 3000: Train loss 0.1733, Val loss 7.1965, Time: 230.48s\n",
            "Step 3250: Train loss 0.1665, Val loss 7.3343, Time: 249.06s\n",
            "Step 3500: Train loss 0.1585, Val loss 7.4599, Time: 267.30s\n",
            "Step 3750: Train loss 0.1487, Val loss 7.6057, Time: 285.73s\n",
            "Step 4000: Train loss 0.1414, Val loss 7.6944, Time: 305.11s\n",
            "Step 4250: Train loss 0.1387, Val loss 7.7234, Time: 323.30s\n",
            "Step 4500: Train loss 0.1346, Val loss 7.8634, Time: 341.61s\n",
            "Step 4750: Train loss 0.1274, Val loss 7.9672, Time: 360.48s\n",
            "Step 4999: Train loss 0.1274, Val loss 7.9480, Time: 378.61s\n",
            "\n",
            "Training finished in 378.66 seconds.\n",
            "Best validation loss achieved: 4.4019\n",
            "Final model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 7: Generation / Inference\n",
        "#------------------------------------\n",
        "\n",
        "# --- Load Model and Vocab ---\n",
        "print(\"Loading model and vocabulary for generation...\")\n",
        "\n",
        "# Load vocabulary\n",
        "try:\n",
        "    with open(config['vocab_path'], 'rb') as f:\n",
        "        saved_vocab = pickle.load(f)\n",
        "        stoi = saved_vocab['stoi']\n",
        "        itos = saved_vocab['itos']\n",
        "    print(f\"Vocabulary loaded from {config['vocab_path']}\")\n",
        "    # Update config with loaded vocab size if necessary (should match)\n",
        "    config['vocab_size'] = len(stoi)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Vocabulary file not found at {config['vocab_path']}. Cannot generate.\")\n",
        "    # Exit or handle error appropriately\n",
        "    stoi, itos = {}, {} # Avoid crashing later code\n",
        "\n",
        "# Re-create model architecture using saved config (or current config if running sequentially)\n",
        "# Important: Make sure the config matches the one used for the saved weights!\n",
        "# If loading a checkpoint, the config is usually saved within it.\n",
        "# Here we assume the 'config' dictionary is still available and matches.\n",
        "if config['vocab_size'] > 0: # Only proceed if vocab loaded\n",
        "    gen_model = GPT(config)\n",
        "    gen_model.to(device)\n",
        "    print(\"Model architecture created.\")\n",
        "\n",
        "    # Load trained weights\n",
        "    try:\n",
        "        checkpoint = torch.load(config['model_path'], map_location=device)\n",
        "        gen_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        gen_model.eval() # Set model to evaluation mode\n",
        "        print(f\"Loaded trained model weights from {config['model_path']}\")\n",
        "\n",
        "        # --- Generate Text ---\n",
        "        print(\"\\n--- Generating Text ---\")\n",
        "\n",
        "        # Starting context (prompt)\n",
        "        # Make sure words are likely in the vocabulary or use <UNK>\n",
        "        start_text = \"My dear Watson,\"\n",
        "        # start_text = \"The game is\"\n",
        "        # start_text = \"It was a dark and stormy\" # May contain words not in vocab if MIN_WORD_FREQ was high\n",
        "\n",
        "        print(f\"Starting prompt: '{start_text}'\")\n",
        "        start_ids = encode(start_text)\n",
        "        # Context needs to be a batch (even if batch size is 1)\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=device).unsqueeze(0) # Add batch dimension\n",
        "\n",
        "        # Generate!\n",
        "        max_tokens_to_generate = 100\n",
        "        temperature = 0.8 # Lower -> less random; Higher -> more random\n",
        "        top_k = 50        # Consider only top 50 words\n",
        "\n",
        "        # Run generation within torch.no_grad() context\n",
        "        with torch.no_grad():\n",
        "            with autocast(enabled=config['use_amp']): # Use AMP for generation too (optional, less critical)\n",
        "                y = gen_model.generate(x, max_tokens_to_generate, temperature=temperature, top_k=top_k)\n",
        "\n",
        "        # Decode the generated sequence\n",
        "        generated_ids = y[0].tolist() # Get the list of IDs from the batch\n",
        "        generated_text = decode(generated_ids)\n",
        "\n",
        "        print(\"\\nGenerated Text:\")\n",
        "        print(generated_text)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Model checkpoint file not found at {config['model_path']}. Train the model first (Snippet 6).\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during generation: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping generation because vocabulary could not be loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZXlZn_QZ5Ub",
        "outputId": "b64ee5bb-c451-4cac-d384-a6b35342d381"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and vocabulary for generation...\n",
            "Vocabulary loaded from /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/word_vocab.pkl\n",
            "Model Parameter Count: 11.96 M\n",
            "Model architecture created.\n",
            "Loaded trained model weights from /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n",
            "\n",
            "--- Generating Text ---\n",
            "Starting prompt: 'My dear Watson,'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-727b98a852a1>:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=config['use_amp']): # Use AMP for generation too (optional, less critical)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "my dear watson , ” “ i shall have your name <UNK> ” “ then i am not <UNK> ” “ oh , i am , ” i asked . “ he remarked for it . “ you think , ” lestrade , “ you would <UNK> ” “ not say that is in so . “ yes , that , or it is very quietly . i shall not be <UNK> ” “ no one is a matter and i shall not think , and have you find your hands. ” “ but the <UNK> ? ” “ they have the <UNK> ”\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----- nano gpt2 training and val loss-------\n",
        "\n",
        "Starting training for 5000 iterations...\n",
        "Starting training from scratch.\n",
        "<ipython-input-7-5a45c5a8127e>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
        "  scaler = GradScaler(enabled=config['use_amp'])\n",
        "<ipython-input-7-5a45c5a8127e>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']): # Enable AMP context\n",
        "Step 0: Train loss 8.1136, Val loss 8.1158, Time: 2.41s\n",
        "New best validation loss: 8.1158. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "<ipython-input-7-5a45c5a8127e>:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Step 250: Train loss 5.5643, Val loss 5.6511, Time: 8.98s\n",
        "New best validation loss: 5.6511. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 500: Train loss 5.0102, Val loss 5.1285, Time: 14.27s\n",
        "New best validation loss: 5.1285. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 750: Train loss 4.6698, Val loss 4.8321, Time: 20.34s\n",
        "New best validation loss: 4.8321. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 1000: Train loss 4.5097, Val loss 4.7091, Time: 25.61s\n",
        "New best validation loss: 4.7091. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 1250: Train loss 4.3960, Val loss 4.6327, Time: 32.04s\n",
        "New best validation loss: 4.6327. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 1500: Train loss 4.2993, Val loss 4.5837, Time: 37.59s\n",
        "New best validation loss: 4.5837. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 1750: Train loss 4.2251, Val loss 4.5558, Time: 43.33s\n",
        "New best validation loss: 4.5558. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 2000: Train loss 4.1589, Val loss 4.5330, Time: 49.61s\n",
        "New best validation loss: 4.5330. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 2250: Train loss 4.1200, Val loss 4.5168, Time: 55.11s\n",
        "New best validation loss: 4.5168. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 2500: Train loss 4.0658, Val loss 4.5007, Time: 61.25s\n",
        "New best validation loss: 4.5007. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 2750: Train loss 4.0075, Val loss 4.4830, Time: 66.65s\n",
        "New best validation loss: 4.4830. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 3000: Train loss 3.9664, Val loss 4.4647, Time: 72.81s\n",
        "New best validation loss: 4.4647. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 3250: Train loss 3.9211, Val loss 4.4585, Time: 78.12s\n",
        "New best validation loss: 4.4585. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 3500: Train loss 3.8668, Val loss 4.4474, Time: 84.09s\n",
        "New best validation loss: 4.4474. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 3750: Train loss 3.8305, Val loss 4.4408, Time: 89.74s\n",
        "New best validation loss: 4.4408. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 4000: Train loss 3.7955, Val loss 4.4431, Time: 95.35s\n",
        "Step 4250: Train loss 3.7554, Val loss 4.4286, Time: 101.25s\n",
        "New best validation loss: 4.4286. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 4500: Train loss 3.7198, Val loss 4.4271, Time: 106.63s\n",
        "New best validation loss: 4.4271. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 4750: Train loss 3.6804, Val loss 4.4246, Time: 112.78s\n",
        "New best validation loss: 4.4246. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "Step 4999: Train loss 3.6473, Val loss 4.4231, Time: 118.12s\n",
        "New best validation loss: 4.4231. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "\n",
        "Training finished in 118.17 seconds.\n",
        "Best validation loss achieved: 4.4231\n",
        "Final model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel/gpt2_word_level.pth\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------micro gpt2 training and val loss------\n",
        "\n",
        "\n",
        "Starting training for 5000 iterations...\n",
        "Starting training from scratch.\n",
        "<ipython-input-12-5a45c5a8127e>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
        "  scaler = GradScaler(enabled=config['use_amp'])\n",
        "<ipython-input-12-5a45c5a8127e>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']): # Enable AMP context\n",
        "Step 0: Train loss 8.1091, Val loss 8.1132, Time: 2.19s\n",
        "New best validation loss: 8.1132. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "<ipython-input-12-5a45c5a8127e>:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Step 250: Train loss 4.6725, Val loss 4.8251, Time: 9.66s\n",
        "New best validation loss: 4.8251. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "Step 500: Train loss 4.3028, Val loss 4.5740, Time: 16.31s\n",
        "New best validation loss: 4.5740. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "Step 750: Train loss 4.0856, Val loss 4.4823, Time: 23.69s\n",
        "New best validation loss: 4.4823. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "Step 1000: Train loss 3.8919, Val loss 4.4057, Time: 30.46s\n",
        "New best validation loss: 4.4057. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "Step 1250: Train loss 3.7193, Val loss 4.3860, Time: 38.10s\n",
        "New best validation loss: 4.3860. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "Step 1500: Train loss 3.5488, Val loss 4.3806, Time: 44.89s\n",
        "New best validation loss: 4.3806. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_micro/gpt2_word_level.pth\n",
        "Step 1750: Train loss 3.4059, Val loss 4.4012, Time: 53.10s\n",
        "Step 2000: Train loss 3.2641, Val loss 4.4179, Time: 60.18s\n",
        "Step 2250: Train loss 3.1045, Val loss 4.4706, Time: 67.02s\n",
        "Step 2500: Train loss 2.9557, Val loss 4.5053, Time: 74.52s\n",
        "Step 2750: Train loss 2.7976, Val loss 4.5889, Time: 81.70s\n",
        "Step 3000: Train loss 2.6644, Val loss 4.6525, Time: 89.00s\n",
        "Step 3250: Train loss 2.5169, Val loss 4.7121, Time: 95.61s\n",
        "Step 3500: Train loss 2.3694, Val loss 4.8029, Time: 102.88s\n",
        "Step 3750: Train loss 2.2329, Val loss 4.8874, Time: 109.60s\n",
        "Step 4000: Train loss 2.0989, Val loss 4.9620, Time: 116.89s\n",
        "Step 4250: Train loss 1.9689, Val loss 5.0316, Time: 123.61s\n",
        "Step 4500: Train loss 1.8493, Val loss 5.1189, Time: 130.93s\n",
        "Step 4750: Train loss 1.7385, Val loss 5.2209, Time: 137.88s\n",
        "Step 4999: Train loss 1.6245, Val loss 5.2630, Time: 144.81s\n",
        "\n",
        "\n",
        "#------------tiny gpt2 training and val loss-------------------------\n",
        "\n",
        "\n",
        "Starting training for 5000 iterations...\n",
        "Starting training from scratch.\n",
        "<ipython-input-13-5a45c5a8127e>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
        "  scaler = GradScaler(enabled=config['use_amp'])\n",
        "<ipython-input-13-5a45c5a8127e>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']): # Enable AMP context\n",
        "Step 0: Train loss 8.2298, Val loss 8.2239, Time: 4.00s\n",
        "New best validation loss: 8.2239. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n",
        "<ipython-input-13-5a45c5a8127e>:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Step 250: Train loss 4.2291, Val loss 4.5362, Time: 25.53s\n",
        "New best validation loss: 4.5362. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n",
        "Step 500: Train loss 3.7176, Val loss 4.4019, Time: 45.14s\n",
        "New best validation loss: 4.4019. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth\n",
        "Step 750: Train loss 3.1563, Val loss 4.4389, Time: 64.19s\n",
        "Step 1000: Train loss 2.4839, Val loss 4.6733, Time: 82.29s\n",
        "Step 1250: Train loss 1.7021, Val loss 5.0798, Time: 100.41s\n",
        "Step 1500: Train loss 1.0136, Val loss 5.5482, Time: 118.78s\n",
        "Step 1750: Train loss 0.5694, Val loss 6.0136, Time: 137.16s\n",
        "Step 2000: Train loss 0.3424, Val loss 6.4148, Time: 156.01s\n",
        "Step 2250: Train loss 0.2554, Val loss 6.7523, Time: 174.22s\n",
        "Step 2500: Train loss 0.2120, Val loss 6.9744, Time: 192.41s\n",
        "Step 2750: Train loss 0.1912, Val loss 7.1061, Time: 212.28s\n",
        "Step 3000: Train loss 0.1733, Val loss 7.1965, Time: 230.48s\n",
        "Step 3250: Train loss 0.1665, Val loss 7.3343, Time: 249.06s\n",
        "Step 3500: Train loss 0.1585, Val loss 7.4599, Time: 267.30s\n",
        "Step 3750: Train loss 0.1487, Val loss 7.6057, Time: 285.73s\n",
        "Step 4000: Train loss 0.1414, Val loss 7.6944, Time: 305.11s\n",
        "Step 4250: Train loss 0.1387, Val loss 7.7234, Time: 323.30s\n",
        "Step 4500: Train loss 0.1346, Val loss 7.8634, Time: 341.61s\n",
        "Step 4750: Train loss 0.1274, Val loss 7.9672, Time: 360.48s\n",
        "Step 4999: Train loss 0.1274, Val loss 7.9480, Time: 378.61s\n",
        "\n",
        "Training finished in 378.66 seconds.\n",
        "Best validation loss achieved: 4.4019\n",
        "Final model saved to /content/drive/MyDrive/gpt2_scratch_wordlevel_tiny/gpt2_word_level.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "3yCssMxOYs9C",
        "outputId": "5815df8c-9e7a-49a3-c63f-19436fe2afa1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-7-8d5439708259>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-8d5439708259>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    <ipython-input-7-5a45c5a8127e>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 1: Setup and Google Drive Mount (Character Level)\n",
        "#------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast # For mixed precision\n",
        "\n",
        "import math\n",
        "import os\n",
        "import requests # For downloading data\n",
        "# import nltk # No longer needed for basic character tokenization\n",
        "from collections import Counter # Still useful for analysis, but not vocab building here\n",
        "import pickle # To save/load vocabulary\n",
        "import time\n",
        "from tqdm import tqdm # Progress bar\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "# !! CHANGE PATHS FOR CHARACTER MODEL !!\n",
        "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/gpt2_scratch_charlevel_tiny\" # CHANGE AS NEEDED\n",
        "if not os.path.exists(DRIVE_SAVE_DIR):\n",
        "    os.makedirs(DRIVE_SAVE_DIR)\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# NLTK download no longer needed for basic char level\n",
        "# try:\n",
        "#     nltk.data.find('tokenizers/punkt')\n",
        "# except nltk.downloader.DownloadError:\n",
        "#     print(\"Downloading NLTK punkt tokenizer...\")\n",
        "#     nltk.download('punkt')\n",
        "#     print(\"NLTK punkt downloaded.\")\n",
        "\n",
        "print(\"Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqeSi3YlaCSC",
        "outputId": "b45b1c47-cfd4-45ec-e614-c2a306cdd9df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 2: Configuration (Character Level)\n",
        "#------------------------------------\n",
        "\n",
        "# --- Model Hyperparameters --- Choose one preset ---\n",
        "# Character models can sometimes benefit from larger embedding dimensions relative\n",
        "# to vocab size, but we keep similar sizes to start.\n",
        "# Preset: 'nano'\n",
        "#n_layer = 3\n",
        "#n_head = 3\n",
        "#n_embd = 48\n",
        "\n",
        "# Preset: 'micro'\n",
        "#n_layer = 4\n",
        "#n_head = 4\n",
        "#n_embd = 128\n",
        "\n",
        "# Preset: 'tiny' (Good starting point for T4)\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384 # Embedding dimension must be divisible by n_head\n",
        "\n",
        "# Preset: 'small' (Monitor memory closely)\n",
        "# n_layer = 12\n",
        "# n_head = 12\n",
        "# n_embd = 768\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "# Character models often use larger block sizes as characters are less informative\n",
        "# than words. Increase if memory allows. 256 is common.\n",
        "block_size = 128      # Max context length (sequence of characters)\n",
        "batch_size = 64       # Can often use slightly larger batch size due to smaller emb table lookup\n",
        "max_iters = 5000      # Total training iterations (might need more for chars)\n",
        "eval_interval = 250   # How often to evaluate\n",
        "learning_rate = 3e-4  # Learning rate (might need tuning)\n",
        "eval_iters = 100      # Number of batches for validation loss avg\n",
        "dropout = 0.1         # Dropout rate\n",
        "use_amp = True        # Use Automatic Mixed Precision\n",
        "\n",
        "# --- Data ---\n",
        "DATA_URL = \"https://www.gutenberg.org/files/1661/1661-0.txt\" # Sherlock Holmes\n",
        "DATA_PATH = \"sherlock_holmes_char.txt\" # Keep data separate if desired\n",
        "# !! CHANGE PATHS FOR CHARACTER MODEL !!\n",
        "VOCAB_PATH = os.path.join(DRIVE_SAVE_DIR, \"char_vocab.pkl\")\n",
        "TRAIN_MODEL_PATH = os.path.join(DRIVE_SAVE_DIR, \"gpt2_char_level.pth\")\n",
        "# MIN_WORD_FREQ = 3     # No longer needed for characters\n",
        "\n",
        "# Derived parameters\n",
        "assert n_embd % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "config = {\n",
        "    'n_layer': n_layer,\n",
        "    'n_head': n_head,\n",
        "    'n_embd': n_embd,\n",
        "    'block_size': block_size,\n",
        "    'batch_size': batch_size,\n",
        "    'max_iters': max_iters,\n",
        "    'eval_interval': eval_interval,\n",
        "    'learning_rate': learning_rate,\n",
        "    'eval_iters': eval_iters,\n",
        "    'dropout': dropout,\n",
        "    'use_amp': use_amp,\n",
        "    'vocab_size': -1, # Will be set after data loading (character vocab size)\n",
        "    'device': device,\n",
        "    # 'min_word_freq': MIN_WORD_FREQ, # Removed\n",
        "    'vocab_path': VOCAB_PATH,\n",
        "    'model_path': TRAIN_MODEL_PATH\n",
        "}\n",
        "\n",
        "print(\"Character-Level Configuration:\")\n",
        "for key, val in config.items():\n",
        "    print(f\"{key}: {val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbZVr3qBaTJs",
        "outputId": "7735b4ff-49aa-4a38-ce73-6eb632c3d5b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character-Level Configuration:\n",
            "n_layer: 6\n",
            "n_head: 6\n",
            "n_embd: 384\n",
            "block_size: 128\n",
            "batch_size: 64\n",
            "max_iters: 5000\n",
            "eval_interval: 250\n",
            "learning_rate: 0.0003\n",
            "eval_iters: 100\n",
            "dropout: 0.1\n",
            "use_amp: True\n",
            "vocab_size: -1\n",
            "device: cuda\n",
            "vocab_path: /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/char_vocab.pkl\n",
            "model_path: /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 3: Data Preparation (Character Level)\n",
        "#------------------------------------\n",
        "\n",
        "# --- Download Data ---\n",
        "# (Using a different DATA_PATH to avoid potential conflicts if needed, but content is the same)\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"Downloading data from {DATA_URL}...\")\n",
        "    try:\n",
        "        response = requests.get(DATA_URL)\n",
        "        response.raise_for_status()\n",
        "        text = response.content.decode('utf-8-sig')\n",
        "        start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "        end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "        start_idx = text.find(start_marker)\n",
        "        if start_idx != -1:\n",
        "            start_idx += len(start_marker)\n",
        "            text = text[start_idx:]\n",
        "        end_idx = text.find(end_marker)\n",
        "        if end_idx != -1:\n",
        "            text = text[:end_idx]\n",
        "        text = text.strip()\n",
        "        print(f\"Data downloaded and saved to {DATA_PATH}. Length: {len(text)} characters.\")\n",
        "        with open(DATA_PATH, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading data: {e}\")\n",
        "        text = \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data processing: {e}\")\n",
        "        text = \"\"\n",
        "else:\n",
        "    print(f\"Data file {DATA_PATH} already exists.\")\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    print(f\"Loaded data from {DATA_PATH}. Length: {len(text)} characters.\")\n",
        "\n",
        "# --- Tokenization & Vocabulary (Character Level) ---\n",
        "print(\"Building character vocabulary...\")\n",
        "# Find all unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "config['vocab_size'] = vocab_size # Update config\n",
        "\n",
        "print(f\"Character vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {''.join(chars)}\") # Print the actual characters\n",
        "\n",
        "# Create mapping from character to integer index and vice-versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "print(f\"Saving vocabulary to {config['vocab_path']}...\")\n",
        "with open(config['vocab_path'], 'wb') as f:\n",
        "    pickle.dump({'stoi': stoi, 'itos': itos}, f)\n",
        "print(\"Vocabulary saved.\")\n",
        "\n",
        "# --- Encode/Decode Functions (Character Level) ---\n",
        "def encode(text_string):\n",
        "    # Convert string to list of character indices\n",
        "    return [stoi[c] for c in text_string] # Assumes chars in text_string are in vocab\n",
        "\n",
        "def decode(indices):\n",
        "    # Convert list of indices back to string\n",
        "    return ''.join([itos[i] for i in indices])\n",
        "\n",
        "# --- Create Data Tensors ---\n",
        "print(\"Encoding entire dataset (character level)...\")\n",
        "full_data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"Encoded data shape: {full_data.shape}\") # Shape will be (total_chars,)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "n = len(full_data)\n",
        "train_data = full_data[:int(n*0.9)]\n",
        "val_data = full_data[int(n*0.9):]\n",
        "\n",
        "print(f\"Train set size: {len(train_data)} characters\")\n",
        "print(f\"Validation set size: {len(val_data)} characters\")\n",
        "\n",
        "# --- Dataset Class (Unchanged logic, works for chars too) ---\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # We can start a sequence at almost any point\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Grab a chunk of data for context (x) and target (y)\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        x = chunk[:-1]\n",
        "        y = chunk[1:]\n",
        "        return x, y\n",
        "\n",
        "# Example usage (optional):\n",
        "# test_dataset = TextDataset(train_data, config['block_size'])\n",
        "# x_ex, y_ex = test_dataset[0]\n",
        "# print(\"Example x:\", x_ex)\n",
        "# print(\"Example y:\", y_ex)\n",
        "# print(\"Decoded x:\", decode(x_ex.tolist()))\n",
        "# print(\"Decoded y:\", decode(y_ex.tolist()))\n",
        "\n",
        "print(\"Character Data Preparation Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VNQf_qcaU_D",
        "outputId": "1bea63fe-090b-4d54-ea15-2a49f306760f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data file sherlock_holmes_char.txt already exists.\n",
            "Loaded data from sherlock_holmes_char.txt. Length: 581421 characters.\n",
            "Building character vocabulary...\n",
            "Character vocabulary size: 97\n",
            "Vocabulary: \n",
            " !#$%&()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz£½àâæèéœ—‘’“”•™\n",
            "Saving vocabulary to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/char_vocab.pkl...\n",
            "Vocabulary saved.\n",
            "Encoding entire dataset (character level)...\n",
            "Encoded data shape: torch.Size([581421])\n",
            "Train set size: 523278 characters\n",
            "Validation set size: 58143 characters\n",
            "Character Data Preparation Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 4: GPT-2 Model Components (Unchanged)\n",
        "#------------------------------------\n",
        "# This snippet defining LayerNorm, CausalSelfAttention, MLP, Block\n",
        "# does NOT need modification. The components operate on embedding\n",
        "# dimensions and sequence lengths, independent of whether the\n",
        "# initial tokens were words or characters.\n",
        "\n",
        "# --- Layer Normalization ---\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "    def __init__(self, ndim, bias=True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        mean = input.mean(dim=-1, keepdim=True)\n",
        "        var = input.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        normalized_input = (input - mean) / torch.sqrt(var + 1e-5)\n",
        "        output = normalized_input * self.weight\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        return output\n",
        "\n",
        "# --- Causal Self-Attention Head ---\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config['n_embd'] % config['n_head'] == 0\n",
        "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
        "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
        "        self.attn_dropout = nn.Dropout(config['dropout'])\n",
        "        self.resid_dropout = nn.Dropout(config['dropout'])\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
        "                                      .view(1, 1, config['block_size'], config['block_size']))\n",
        "        self.n_head = config['n_head']\n",
        "        self.n_embd = config['n_embd']\n",
        "        self.dropout = config['dropout']\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        head_size = C // self.n_head\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_size))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # (B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "# --- Feed Forward Network (MLP) ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config['n_embd'], 4 * config['n_embd'])\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config['n_embd'], config['n_embd'])\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "# --- Transformer Block ---\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config['n_embd'])\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config['n_embd'])\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # Attention path\n",
        "        x = x + self.mlp(self.ln_2(x))  # MLP path\n",
        "        return x\n",
        "\n",
        "print(\"Model Components Defined (Unchanged - LayerNorm, CausalSelfAttention, MLP, Block).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--flGnePadG7",
        "outputId": "c019384b-a3f0-4a6e-a3c1-826e55fc6820"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Components Defined (Unchanged - LayerNorm, CausalSelfAttention, MLP, Block).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 5: Full GPT-2 Model\n",
        "#------------------------------------\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            # Token embeddings\n",
        "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
        "            # Positional embeddings (learned)\n",
        "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
        "            # Dropout after embedding + positional encoding\n",
        "            drop = nn.Dropout(config['dropout']),\n",
        "            # Stack of transformer blocks\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
        "            # Final layer normalization before the output head\n",
        "            ln_f = LayerNorm(config['n_embd']), # Or nn.LayerNorm(config['n_embd'])\n",
        "        ))\n",
        "        # Language modeling head (maps embeddings to vocabulary logits)\n",
        "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Tie the weights between the token embeddings and the final linear layer\n",
        "        # This improves performance and reduces parameters\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize weights (important for transformer stability)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config['n_layer']))\n",
        "\n",
        "        # Report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Model Parameter Count: {n_params/1e6:.2f} M\")\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        # Initialize Linear and Embedding layers\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size() # Batch size, sequence length\n",
        "        assert t <= self.config['block_size'], f\"Cannot forward sequence of length {t}, block size is only {self.config['block_size']}\"\n",
        "\n",
        "        # --- Forward pass through the transformer ---\n",
        "        # 1. Get token embeddings\n",
        "        tok_emb = self.transformer.wte(idx) # Shape: (b, t, n_embd)\n",
        "\n",
        "        # 2. Get positional embeddings\n",
        "        # Create position IDs: tensor of [0, 1, ..., t-1]\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # Shape: (1, t)\n",
        "        pos_emb = self.transformer.wpe(pos) # Shape: (1, t, n_embd)\n",
        "\n",
        "        # 3. Add token and positional embeddings\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # 4. Pass through transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        # 5. Final layer normalization\n",
        "        x = self.transformer.ln_f(x) # Shape: (b, t, n_embd)\n",
        "\n",
        "        # --- Language Modeling Head ---\n",
        "        if targets is not None:\n",
        "            # If we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x) # Shape: (b, t, vocab_size)\n",
        "            # Calculate loss using cross-entropy\n",
        "            # Need to reshape for CrossEntropyLoss: expects (N, C) and (N)\n",
        "            # N = b * t, C = vocab_size\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0) # Ignore PAD token if targets contain it\n",
        "        else:\n",
        "            # Inference-time configuration: only forward the lm_head on the very last position\n",
        "            # This is slightly more efficient during generation\n",
        "            logits = self.lm_head(x[:, [-1], :]) # Note: using list [-1] keeps the T dimension\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # --- Generation Method ---\n",
        "    @torch.no_grad() # IMPORTANT: Disable gradient calculation during generation\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # If the sequence context is growing too long, crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
        "\n",
        "            # Forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond) # We don't need the loss here\n",
        "\n",
        "            # Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature # Shape: (b, vocab_size)\n",
        "\n",
        "            # Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                # Set logits not in the top k to -infinity\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # Shape: (b, vocab_size)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # Shape: (b, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # Shape: (b, t+1)\n",
        "\n",
        "        self.train() # Set model back to train mode if needed later\n",
        "        return idx\n",
        "\n",
        "# --- Instantiate the model ---\n",
        "# Ensure vocab size is set in config\n",
        "if config['vocab_size'] == -1:\n",
        "     raise ValueError(\"Vocabulary size not set. Run Snippet 3 first.\")\n",
        "\n",
        "model = GPT(config)\n",
        "model.to(device)\n",
        "\n",
        "print(\"GPT Model Instantiated.\")\n",
        "# Optional: Print model structure\n",
        "# print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6usGPu93af79",
        "outputId": "cf25f666-825c-4fb7-f0c7-812f1aa742ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Parameter Count: 10.73 M\n",
            "GPT Model Instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 6: Training Loop (Using Char Paths)\n",
        "#------------------------------------\n",
        "\n",
        "# --- DataLoader ---\n",
        "# Using the same TextDataset class, but with character data\n",
        "train_dataset = TextDataset(train_data, config['block_size'])\n",
        "val_dataset = TextDataset(val_data, config['block_size'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --- Optimizer ---\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "# --- Mixed Precision Scaler ---\n",
        "scaler = GradScaler(enabled=config['use_amp'])\n",
        "\n",
        "# --- Estimate Validation Loss Function (Unchanged Logic) ---\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model_to_eval, eval_iters):\n",
        "    out = {}\n",
        "    model_to_eval.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        loader = train_loader if split == 'train' else val_loader\n",
        "        loader_iter = iter(loader)\n",
        "        for k in range(eval_iters):\n",
        "            try: X, Y = next(loader_iter)\n",
        "            except StopIteration:\n",
        "                 loader_iter = iter(loader)\n",
        "                 X, Y = next(loader_iter)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            with autocast(enabled=config['use_amp']):\n",
        "                 logits, loss = model_to_eval(X, Y)\n",
        "            # Check for valid loss (not NaN or Inf)\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"Warning: Encountered {loss.item()} loss in {split} evaluation step {k}. Skipping.\")\n",
        "                losses[k] = losses[k-1] if k > 0 else 50.0 # Use previous or a high value\n",
        "            else:\n",
        "                losses[k] = loss.item()\n",
        "        out[split] = losses[losses != 0].mean() # Avoid averaging zeros if issues occurred\n",
        "    model_to_eval.train()\n",
        "    return out\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(f\"Starting character-level training for {config['max_iters']} iterations...\")\n",
        "best_val_loss = float('inf')\n",
        "start_time = time.time()\n",
        "train_iter = iter(train_loader) # Initialize train iterator\n",
        "\n",
        "# Resume from checkpoint if exists (using the CHARACTER model path)\n",
        "if os.path.exists(config['model_path']):\n",
        "    print(f\"Resuming training from checkpoint: {config['model_path']}\")\n",
        "    try:\n",
        "        checkpoint = torch.load(config['model_path'], map_location=device)\n",
        "        # Ensure loaded config vocab size matches current config\n",
        "        # This is a basic check; more robust checks might compare more config keys\n",
        "        if checkpoint['config']['vocab_size'] != config['vocab_size']:\n",
        "             print(f\"Warning: Checkpoint vocab size ({checkpoint['config']['vocab_size']}) differs \"\n",
        "                   f\"from current config ({config['vocab_size']}). Loading weights may fail or lead to errors.\")\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # Load optimizer state if needed (recommended for longer training)\n",
        "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        # best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "        # start_iter = checkpoint.get('iter', 0) + 1\n",
        "        print(\"Loaded model weights. Re-initializing optimizer/iteration count for simplicity.\")\n",
        "        start_iter = 0 # Or load from checkpoint if desired\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}. Starting training from scratch.\")\n",
        "        start_iter = 0\n",
        "else:\n",
        "    print(\"Starting training from scratch.\")\n",
        "    start_iter = 0\n",
        "\n",
        "\n",
        "for iter_num in range(start_iter, config['max_iters']):\n",
        "    # Evaluation\n",
        "    if iter_num > 0 and (iter_num % config['eval_interval'] == 0 or iter_num == config['max_iters'] - 1):\n",
        "        losses = estimate_loss(model, config['eval_iters'])\n",
        "        current_time = time.time()\n",
        "        elapsed_time = current_time - start_time\n",
        "        print(f\"Step {iter_num}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}, Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save checkpoint (using CHARACTER model path)\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            print(f\"New best validation loss: {best_val_loss:.4f}. Saving model...\")\n",
        "            checkpoint = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'config': config, # Save config used for this checkpoint\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'iter': iter_num\n",
        "            }\n",
        "            torch.save(checkpoint, config['model_path'])\n",
        "            print(f\"Model saved to {config['model_path']}\")\n",
        "\n",
        "    # Sample batch\n",
        "    try:\n",
        "        X_batch, Y_batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        X_batch, Y_batch = next(train_iter)\n",
        "\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    # Forward/Backward pass with AMP\n",
        "    with autocast(enabled=config['use_amp']):\n",
        "        logits, loss = model(X_batch, Y_batch)\n",
        "\n",
        "    if torch.isnan(loss) or torch.isinf(loss):\n",
        "        print(f\"Warning: NaN or Inf loss detected at iteration {iter_num}. Skipping backward step.\")\n",
        "        optimizer.zero_grad(set_to_none=True) # Still zero grads\n",
        "        continue # Skip optimizer step and scaler update if loss is invalid\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    scaler.scale(loss).backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Optional clipping\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Optional LR scheduler step here\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTraining finished in {end_time - start_time:.2f} seconds.\")\n",
        "print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
        "print(f\"Final character model saved to {config['model_path']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "debsW20BahFs",
        "outputId": "11e70d89-d44c-4403-c068-7e256812d4a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting character-level training for 5000 iterations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-76ccea85ce5a>:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=config['use_amp'])\n",
            "<ipython-input-23-76ccea85ce5a>:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=config['use_amp']):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training from scratch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-76ccea85ce5a>:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=config['use_amp']):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250: Train loss 2.1820, Val loss 2.2673, Time: 31.31s\n",
            "New best validation loss: 2.2673. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 500: Train loss 1.7160, Val loss 1.7539, Time: 62.38s\n",
            "New best validation loss: 1.7539. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 750: Train loss 1.4598, Val loss 1.5471, Time: 93.20s\n",
            "New best validation loss: 1.5471. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 1000: Train loss 1.3112, Val loss 1.4290, Time: 124.36s\n",
            "New best validation loss: 1.4290. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 1250: Train loss 1.2058, Val loss 1.3561, Time: 155.27s\n",
            "New best validation loss: 1.3561. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 1500: Train loss 1.1288, Val loss 1.3267, Time: 186.15s\n",
            "New best validation loss: 1.3267. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 1750: Train loss 1.0602, Val loss 1.3150, Time: 217.42s\n",
            "New best validation loss: 1.3150. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 2000: Train loss 0.9976, Val loss 1.3072, Time: 248.26s\n",
            "New best validation loss: 1.3072. Saving model...\n",
            "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "Step 2250: Train loss 0.9275, Val loss 1.3082, Time: 279.19s\n",
            "Step 2500: Train loss 0.8579, Val loss 1.3319, Time: 309.53s\n",
            "Step 2750: Train loss 0.7940, Val loss 1.3464, Time: 339.82s\n",
            "Step 3000: Train loss 0.7223, Val loss 1.3872, Time: 370.37s\n",
            "Step 3250: Train loss 0.6523, Val loss 1.4332, Time: 400.78s\n",
            "Step 3500: Train loss 0.5894, Val loss 1.4881, Time: 431.11s\n",
            "Step 3750: Train loss 0.5221, Val loss 1.5423, Time: 461.64s\n",
            "Step 4000: Train loss 0.4595, Val loss 1.6109, Time: 492.05s\n",
            "Step 4250: Train loss 0.4077, Val loss 1.6873, Time: 522.32s\n",
            "Step 4500: Train loss 0.3642, Val loss 1.7249, Time: 552.82s\n",
            "Step 4750: Train loss 0.3190, Val loss 1.8078, Time: 583.23s\n",
            "Step 4999: Train loss 0.2891, Val loss 1.8470, Time: 613.53s\n",
            "\n",
            "Training finished in 613.62 seconds.\n",
            "Best validation loss achieved: 1.3072\n",
            "Final character model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------\n",
        "# Snippet 7: Generation / Inference (Character Level)\n",
        "#------------------------------------\n",
        "\n",
        "# --- Load Model and Vocab ---\n",
        "print(\"Loading character model and vocabulary for generation...\")\n",
        "\n",
        "# Load CHARACTER vocabulary\n",
        "try:\n",
        "    with open(config['vocab_path'], 'rb') as f: # Uses char vocab path from config\n",
        "        saved_vocab = pickle.load(f)\n",
        "        stoi = saved_vocab['stoi']\n",
        "        itos = saved_vocab['itos']\n",
        "    print(f\"Character vocabulary loaded from {config['vocab_path']}\")\n",
        "    loaded_vocab_size = len(stoi)\n",
        "    # Update config vocab size based on loaded vocab if needed (should match)\n",
        "    if config['vocab_size'] != loaded_vocab_size:\n",
        "        print(f\"Updating config vocab size from {config['vocab_size']} to loaded {loaded_vocab_size}\")\n",
        "        config['vocab_size'] = loaded_vocab_size\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Character vocabulary file not found at {config['vocab_path']}. Cannot generate.\")\n",
        "    stoi, itos = {}, {} # Avoid crashing\n",
        "    config['vocab_size'] = 0 # Indicate vocab load failure\n",
        "\n",
        "# Re-create model architecture using config\n",
        "# Ensure config matches the saved model's config (esp. vocab_size)\n",
        "if config['vocab_size'] > 0:\n",
        "    gen_model = GPT(config)\n",
        "    gen_model.to(device)\n",
        "    print(\"Model architecture created.\")\n",
        "\n",
        "    # Load trained CHARACTER weights\n",
        "    try:\n",
        "        checkpoint = torch.load(config['model_path'], map_location=device) # Uses char model path\n",
        "\n",
        "        # Optional: Load config from checkpoint for robustness\n",
        "        # loaded_config = checkpoint['config']\n",
        "        # gen_model = GPT(loaded_config) # Re-create model with exact saved config\n",
        "        # gen_model.to(device)\n",
        "        # print(\"Model architecture created from checkpoint config.\")\n",
        "\n",
        "        gen_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        gen_model.eval()\n",
        "        print(f\"Loaded trained character model weights from {config['model_path']}\")\n",
        "\n",
        "        # --- Generate Text ---\n",
        "        print(\"\\n--- Generating Character-Level Text ---\")\n",
        "\n",
        "        # Starting context (prompt) - must only contain characters from the vocab\n",
        "        start_text = \"Sherlock Holmes was\"\n",
        "        # start_text = \"My dear Watson\"\n",
        "        # start_text = \"It was a\"\n",
        "\n",
        "        print(f\"Starting prompt: '{start_text}'\")\n",
        "        # Use the character encode function\n",
        "        start_ids = encode(start_text)\n",
        "        x = torch.tensor(start_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        # Generate! Increase max_tokens_to_generate for characters.\n",
        "        max_tokens_to_generate = 500 # Generate more characters\n",
        "        temperature = 0.75          # Adjust temperature as needed\n",
        "        top_k = 40                  # Adjust top-k as needed\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast(enabled=config['use_amp']):\n",
        "                y = gen_model.generate(x, max_tokens_to_generate, temperature=temperature, top_k=top_k)\n",
        "\n",
        "        # Decode the generated sequence using character decoder\n",
        "        generated_ids = y[0].tolist()\n",
        "        generated_text = decode(generated_ids) # Decodes directly to characters\n",
        "\n",
        "        print(\"\\nGenerated Text:\")\n",
        "        print(generated_text)\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Character model checkpoint file not found at {config['model_path']}. Train the model first (Snippet 6).\")\n",
        "    except KeyError as e:\n",
        "         print(f\"ERROR: Mismatch between loaded checkpoint and model architecture (KeyError: {e}). \"\n",
        "               \"Ensure the config used for generation matches the training config.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during generation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Skipping generation because character vocabulary could not be loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgrPmKZ5d-PS",
        "outputId": "9d9df879-b727-4789-b0eb-e7ce46d2dc42"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading character model and vocabulary for generation...\n",
            "Character vocabulary loaded from /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/char_vocab.pkl\n",
            "Model Parameter Count: 10.73 M\n",
            "Model architecture created.\n",
            "Loaded trained character model weights from /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
            "\n",
            "--- Generating Character-Level Text ---\n",
            "Starting prompt: 'Sherlock Holmes was'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-dc239d5b646a>:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=config['use_amp']):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "Sherlock Holmes was a man that I had no fine of my own way in it. I want to reply you, your case I have been to be.” He see his clack hands before his with a passage of character. “And how did you didn’t know a likely to have them, Mr. Holder?” said he. “Your case, and what means myself does in a particular chair. There is a nine of peint. You don’t know that this breakfast-tain which is warning with it.” He took up a page in one hydroch of his hair with a crimple few holdings, so we have disposed from the facts o\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---- nano ----\n",
        "Starting character-level training for 5000 iterations...\n",
        "<ipython-input-13-76ccea85ce5a>:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
        "  scaler = GradScaler(enabled=config['use_amp'])\n",
        "<ipython-input-13-76ccea85ce5a>:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Starting training from scratch.\n",
        "<ipython-input-13-76ccea85ce5a>:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Step 250: Train loss 2.7152, Val loss 2.7607, Time: 5.76s\n",
        "New best validation loss: 2.7607. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 500: Train loss 2.4923, Val loss 2.5561, Time: 11.37s\n",
        "New best validation loss: 2.5561. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 750: Train loss 2.4134, Val loss 2.4872, Time: 17.52s\n",
        "New best validation loss: 2.4872. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 1000: Train loss 2.3665, Val loss 2.4399, Time: 22.98s\n",
        "New best validation loss: 2.4399. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 1250: Train loss 2.2930, Val loss 2.3684, Time: 29.34s\n",
        "New best validation loss: 2.3684. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 1500: Train loss 2.2239, Val loss 2.2957, Time: 34.74s\n",
        "New best validation loss: 2.2957. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 1750: Train loss 2.1572, Val loss 2.2205, Time: 40.98s\n",
        "New best validation loss: 2.2205. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 2000: Train loss 2.0883, Val loss 2.1549, Time: 47.28s\n",
        "New best validation loss: 2.1549. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 2250: Train loss 2.0420, Val loss 2.1034, Time: 53.59s\n",
        "New best validation loss: 2.1034. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 2500: Train loss 1.9959, Val loss 2.0562, Time: 59.03s\n",
        "New best validation loss: 2.0562. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 2750: Train loss 1.9591, Val loss 2.0119, Time: 64.99s\n",
        "New best validation loss: 2.0119. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 3000: Train loss 1.9238, Val loss 1.9800, Time: 70.74s\n",
        "New best validation loss: 1.9800. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 3250: Train loss 1.8994, Val loss 1.9518, Time: 76.17s\n",
        "New best validation loss: 1.9518. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 3500: Train loss 1.8701, Val loss 1.9168, Time: 82.88s\n",
        "New best validation loss: 1.9168. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 3750: Train loss 1.8524, Val loss 1.8998, Time: 88.38s\n",
        "New best validation loss: 1.8998. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 4000: Train loss 1.8321, Val loss 1.8812, Time: 94.63s\n",
        "New best validation loss: 1.8812. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 4250: Train loss 1.8113, Val loss 1.8606, Time: 100.06s\n",
        "New best validation loss: 1.8606. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 4500: Train loss 1.7933, Val loss 1.8377, Time: 106.36s\n",
        "New best validation loss: 1.8377. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 4750: Train loss 1.7806, Val loss 1.8230, Time: 111.82s\n",
        "New best validation loss: 1.8230. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "Step 4999: Train loss 1.7624, Val loss 1.8109, Time: 117.99s\n",
        "New best validation loss: 1.8109. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "\n",
        "Training finished in 118.07 seconds.\n",
        "Best validation loss achieved: 1.8109\n",
        "Final character model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_nano/gpt2_char_level.pth\n",
        "\n",
        "#---------micro--------------\n",
        "Starting character-level training for 5000 iterations...\n",
        "<ipython-input-6-76ccea85ce5a>:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
        "  scaler = GradScaler(enabled=config['use_amp'])\n",
        "<ipython-input-6-76ccea85ce5a>:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Starting training from scratch.\n",
        "<ipython-input-6-76ccea85ce5a>:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Step 250: Train loss 2.4005, Val loss 2.4682, Time: 9.72s\n",
        "New best validation loss: 2.4682. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 500: Train loss 2.1778, Val loss 2.2607, Time: 18.42s\n",
        "New best validation loss: 2.2607. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 750: Train loss 1.9528, Val loss 2.0107, Time: 29.18s\n",
        "New best validation loss: 2.0107. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 1000: Train loss 1.8069, Val loss 1.8660, Time: 40.45s\n",
        "New best validation loss: 1.8660. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 1250: Train loss 1.7006, Val loss 1.7459, Time: 53.30s\n",
        "New best validation loss: 1.7459. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 1500: Train loss 1.6125, Val loss 1.6709, Time: 64.34s\n",
        "New best validation loss: 1.6709. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 1750: Train loss 1.5468, Val loss 1.6165, Time: 74.02s\n",
        "New best validation loss: 1.6165. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 2000: Train loss 1.4991, Val loss 1.5807, Time: 84.97s\n",
        "New best validation loss: 1.5807. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 2250: Train loss 1.4537, Val loss 1.5423, Time: 95.82s\n",
        "New best validation loss: 1.5423. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 2500: Train loss 1.4188, Val loss 1.5158, Time: 105.18s\n",
        "New best validation loss: 1.5158. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 2750: Train loss 1.3765, Val loss 1.4844, Time: 113.92s\n",
        "New best validation loss: 1.4844. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 3000: Train loss 1.3579, Val loss 1.4723, Time: 123.36s\n",
        "New best validation loss: 1.4723. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 3250: Train loss 1.3325, Val loss 1.4458, Time: 132.51s\n",
        "New best validation loss: 1.4458. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 3500: Train loss 1.3128, Val loss 1.4284, Time: 141.68s\n",
        "New best validation loss: 1.4284. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 3750: Train loss 1.2939, Val loss 1.4247, Time: 150.95s\n",
        "New best validation loss: 1.4247. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 4000: Train loss 1.2784, Val loss 1.4114, Time: 160.23s\n",
        "New best validation loss: 1.4114. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 4250: Train loss 1.2635, Val loss 1.3985, Time: 169.50s\n",
        "New best validation loss: 1.3985. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 4500: Train loss 1.2510, Val loss 1.3881, Time: 178.32s\n",
        "New best validation loss: 1.3881. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 4750: Train loss 1.2382, Val loss 1.3793, Time: 187.29s\n",
        "New best validation loss: 1.3793. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "Step 4999: Train loss 1.2219, Val loss 1.3679, Time: 196.48s\n",
        "New best validation loss: 1.3679. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "\n",
        "Training finished in 196.58 seconds.\n",
        "Best validation loss achieved: 1.3679\n",
        "Final character model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_micro/gpt2_char_level.pth\n",
        "\n",
        "\n",
        "\n",
        "# -----------tiny--------------------\n",
        "\n",
        "Starting character-level training for 5000 iterations...\n",
        "<ipython-input-23-76ccea85ce5a>:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
        "  scaler = GradScaler(enabled=config['use_amp'])\n",
        "<ipython-input-23-76ccea85ce5a>:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Starting training from scratch.\n",
        "<ipython-input-23-76ccea85ce5a>:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
        "  with autocast(enabled=config['use_amp']):\n",
        "Step 250: Train loss 2.1820, Val loss 2.2673, Time: 31.31s\n",
        "New best validation loss: 2.2673. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 500: Train loss 1.7160, Val loss 1.7539, Time: 62.38s\n",
        "New best validation loss: 1.7539. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 750: Train loss 1.4598, Val loss 1.5471, Time: 93.20s\n",
        "New best validation loss: 1.5471. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 1000: Train loss 1.3112, Val loss 1.4290, Time: 124.36s\n",
        "New best validation loss: 1.4290. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 1250: Train loss 1.2058, Val loss 1.3561, Time: 155.27s\n",
        "New best validation loss: 1.3561. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 1500: Train loss 1.1288, Val loss 1.3267, Time: 186.15s\n",
        "New best validation loss: 1.3267. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 1750: Train loss 1.0602, Val loss 1.3150, Time: 217.42s\n",
        "New best validation loss: 1.3150. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 2000: Train loss 0.9976, Val loss 1.3072, Time: 248.26s\n",
        "New best validation loss: 1.3072. Saving model...\n",
        "Model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth\n",
        "Step 2250: Train loss 0.9275, Val loss 1.3082, Time: 279.19s\n",
        "Step 2500: Train loss 0.8579, Val loss 1.3319, Time: 309.53s\n",
        "Step 2750: Train loss 0.7940, Val loss 1.3464, Time: 339.82s\n",
        "Step 3000: Train loss 0.7223, Val loss 1.3872, Time: 370.37s\n",
        "Step 3250: Train loss 0.6523, Val loss 1.4332, Time: 400.78s\n",
        "Step 3500: Train loss 0.5894, Val loss 1.4881, Time: 431.11s\n",
        "Step 3750: Train loss 0.5221, Val loss 1.5423, Time: 461.64s\n",
        "Step 4000: Train loss 0.4595, Val loss 1.6109, Time: 492.05s\n",
        "Step 4250: Train loss 0.4077, Val loss 1.6873, Time: 522.32s\n",
        "Step 4500: Train loss 0.3642, Val loss 1.7249, Time: 552.82s\n",
        "Step 4750: Train loss 0.3190, Val loss 1.8078, Time: 583.23s\n",
        "Step 4999: Train loss 0.2891, Val loss 1.8470, Time: 613.53s\n",
        "\n",
        "Training finished in 613.62 seconds.\n",
        "Best validation loss achieved: 1.3072\n",
        "Final character model saved to /content/drive/MyDrive/gpt2_scratch_charlevel_tiny/gpt2_char_level.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "W4TLvsqag89a",
        "outputId": "5601b836-e821-4d03-bf76-3414e47f6efc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-25-ec81f8f81ffb>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-ec81f8f81ffb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    <ipython-input-13-76ccea85ce5a>:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxPLmO0qh-k7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}